{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. 준비운동\n",
    "- 필요한 라이브러리들을 불러옵니다.\n",
    "- 각각의 클래스의 label(`:int`)을 지정합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "import torch\n",
    "from typing import Dict, Iterable, List, Any, Tuple\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "from sklearn.metrics import f1_score, accuracy_score, auc, roc_curve\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "CLS2IDX = {\n",
    "    'no_relation': 0,\n",
    "    'org:member_of': 1,\n",
    "    'org:top_members/employees': 2,\n",
    "    'org:alternate_names': 3,\n",
    "    'per:date_of_birth': 4,\n",
    "    'org:place_of_headquarters': 5,\n",
    "    'per:employee_of': 6,\n",
    "    'per:origin': 7,\n",
    "    'per:title': 8,\n",
    "    'org:members': 9,\n",
    "    'per:schools_attended': 10,\n",
    "    'per:colleagues': 11,\n",
    "    'per:alternate_names': 12,\n",
    "    'per:spouse': 13,\n",
    "    'org:founded_by': 14,\n",
    "    'org:political/religious_affiliation': 15,\n",
    "    'per:children': 16,\n",
    "    'org:founded': 17,\n",
    "    'org:number_of_employees/members': 18,\n",
    "    'per:place_of_birth': 19,\n",
    "    'org:dissolved': 20,\n",
    "    'per:parents': 21,\n",
    "    'per:religion': 22,\n",
    "    'per:date_of_death': 23,\n",
    "    'per:place_of_residence': 24,\n",
    "    'per:other_family': 25,\n",
    "    'org:product': 26,\n",
    "    'per:siblings': 27,\n",
    "    'per:product': 28,\n",
    "    'per:place_of_death': 29,\n",
    "}\n",
    "IDX2CLS = {\n",
    "    0: 'no_relation',\n",
    "    1: 'org:member_of',\n",
    "    2: 'org:top_members/employees',\n",
    "    3: 'org:alternate_names',\n",
    "    4: 'per:date_of_birth',\n",
    "    5: 'org:place_of_headquarters',\n",
    "    6: 'per:employee_of',\n",
    "    7: 'per:origin',\n",
    "    8: 'per:title',\n",
    "    9: 'org:members',\n",
    "    10: 'per:schools_attended',\n",
    "    11: 'per:colleagues',\n",
    "    12: 'per:alternate_names',\n",
    "    13: 'per:spouse',               # obj, sbj 스왑 가능\n",
    "    14: 'org:founded_by',\n",
    "    15: 'org:political/religious_affiliation',\n",
    "    16: 'per:children',\n",
    "    17: 'org:founded',\n",
    "    18: 'org:number_of_employees/members',\n",
    "    19: 'per:place_of_birth',\n",
    "    20: 'org:dissolved',\n",
    "    21: 'per:parents',              #<-> children\n",
    "    22: 'per:religion',\n",
    "    23: 'per:date_of_death',\n",
    "    24: 'per:place_of_residence',\n",
    "    25: 'per:other_family',\n",
    "    26: 'org:product',\n",
    "    27: 'per:siblings',\n",
    "    28: 'per:product',\n",
    "    29: 'per:place_of_death'\n",
    "}    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-05 07:06:48.519367: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Model과 Tokenizer 설정하기\n",
    "\n",
    "- 허깅페이스 모델 허브에서 가져올 MODEL_NAME을 지정합니다.\n",
    "- model의 구조를 파악해봅니다.\n",
    "    - 특히 마지막 classifer 레이어의 구조를 우리가 원하는 클래시피케이션에 맞게 조정해야합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "MODEL_NAME =  'kykim/bert-kor-base' #\"ainize/klue-bert-base-mrc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained( MODEL_NAME )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- classifier를 우리의 task에 맞게 조정해줍시다.\n",
    "    - `nn.Linear(768, 2, bias=True) -> nn.Linear(768, 30, bias=True)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 위의 구조에 맞춰 파인튜닝해줍시다.\n",
    "model.classifier = torch.nn.Linear(768, 30, bias=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 데이터 \n",
    "\n",
    "- **df** `(DataFrame)` :\n",
    "    - **train.csv**파일을 데이터프레임으로 불러온 것\n",
    "- **df_dict** `(Dict[int, DataFrame])` :\n",
    "    - **df**를 각각의 label별로 분류한 딕셔너리\n",
    "- **df_dict_len** `(Dict[int, int])` :\n",
    "    - 각각의 label별 데이터의 개수(row의 개수)를 분류한 딕셔너리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df = pd.read_csv('../dataset/train/train.csv')\n",
    "df['subject_entity'] = df['subject_entity'].map(lambda x: eval(x)['word'])\n",
    "df['object_entity'] = df['object_entity'].map(lambda x: eval(x)['word'])\n",
    "df['label'] = df['label'].map(lambda x : CLS2IDX[x])\n",
    "df_dict = { i : df.loc[ df[\"label\"] == i , : ].reset_index(drop=True) for i in range(30)}\n",
    "df_dict_len = { i : len(df_dict[i]) for i in range(30)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_dict[11] #label이 11인 데이터만 가져온다 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>1971년 대선을 앞두고 김종필은 1971년 선거에서 박정희 당선을 위해 무려 60...</td>\n",
       "      <td>김종필</td>\n",
       "      <td>박정희</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157</td>\n",
       "      <td>아나킨 스카이워커는 미디클로리언 수치가 20000이 넘는, 선천적으로 가진 포스의 ...</td>\n",
       "      <td>아나킨 스카이워커</td>\n",
       "      <td>콰이곤 진</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276</td>\n",
       "      <td>《Riding with the King》은 2000년 발매된 에릭 클랩튼과 B.B....</td>\n",
       "      <td>B.B. 킹</td>\n",
       "      <td>에릭 클랩튼</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>282</td>\n",
       "      <td>2017년 문재인 정부의 정현백 장관은 \"여성혐오에 소극적으로 대처하는 건 더이상 ...</td>\n",
       "      <td>정현백</td>\n",
       "      <td>문재인</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>요한 바오로 2세는 요제프 라칭거 추기경(훗날의 교황 베네딕토 16세)과 함께 자유...</td>\n",
       "      <td>베네딕토 16세</td>\n",
       "      <td>요한 바오로 2세</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>32188</td>\n",
       "      <td>그 결과로서 밴스는 존 F. 케네디와 린든 B. 존슨 행정부에서 직위들의 계승에 근...</td>\n",
       "      <td>린든 B. 존슨</td>\n",
       "      <td>존 F. 케네디</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>32277</td>\n",
       "      <td>한편 김대중은 김종필, 박태준과 단일화에 합의하였고, 이에 불만을 품은 신한국당 역...</td>\n",
       "      <td>박태준</td>\n",
       "      <td>김대중</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>32284</td>\n",
       "      <td>초반에는 도쿄에서 한 마을인 히나미자와 마을에 전학 온지 얼마 안된 소년 마에바라 ...</td>\n",
       "      <td>마에바라 케이이치</td>\n",
       "      <td>히나미자와</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>32328</td>\n",
       "      <td>문 대통령은 이어진 환담 자리에서 싱 대사에게 지난 해 방중 시 시진핑 주석과 리커...</td>\n",
       "      <td>시진핑</td>\n",
       "      <td>리커창</td>\n",
       "      <td>11</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>32466</td>\n",
       "      <td>법포는 다시 최시형, 서병학, 손병희 직계인 북접과 다시 서장옥, 전봉준, 김개남을...</td>\n",
       "      <td>최시형</td>\n",
       "      <td>손병희</td>\n",
       "      <td>11</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           sentence subject_entity  \\\n",
       "0       20  1971년 대선을 앞두고 김종필은 1971년 선거에서 박정희 당선을 위해 무려 60...            김종필   \n",
       "1      157  아나킨 스카이워커는 미디클로리언 수치가 20000이 넘는, 선천적으로 가진 포스의 ...      아나킨 스카이워커   \n",
       "2      276  《Riding with the King》은 2000년 발매된 에릭 클랩튼과 B.B....         B.B. 킹   \n",
       "3      282  2017년 문재인 정부의 정현백 장관은 \"여성혐오에 소극적으로 대처하는 건 더이상 ...            정현백   \n",
       "4      321  요한 바오로 2세는 요제프 라칭거 추기경(훗날의 교황 베네딕토 16세)과 함께 자유...       베네딕토 16세   \n",
       "..     ...                                                ...            ...   \n",
       "529  32188  그 결과로서 밴스는 존 F. 케네디와 린든 B. 존슨 행정부에서 직위들의 계승에 근...       린든 B. 존슨   \n",
       "530  32277  한편 김대중은 김종필, 박태준과 단일화에 합의하였고, 이에 불만을 품은 신한국당 역...            박태준   \n",
       "531  32284  초반에는 도쿄에서 한 마을인 히나미자와 마을에 전학 온지 얼마 안된 소년 마에바라 ...      마에바라 케이이치   \n",
       "532  32328  문 대통령은 이어진 환담 자리에서 싱 대사에게 지난 해 방중 시 시진핑 주석과 리커...            시진핑   \n",
       "533  32466  법포는 다시 최시형, 서병학, 손병희 직계인 북접과 다시 서장옥, 전봉준, 김개남을...            최시형   \n",
       "\n",
       "    object_entity  label     source  \n",
       "0             박정희     11  wikipedia  \n",
       "1           콰이곤 진     11  wikipedia  \n",
       "2          에릭 클랩튼     11  wikipedia  \n",
       "3             문재인     11  wikipedia  \n",
       "4       요한 바오로 2세     11  wikipedia  \n",
       "..            ...    ...        ...  \n",
       "529      존 F. 케네디     11  wikipedia  \n",
       "530           김대중     11  wikipedia  \n",
       "531         히나미자와     11  wikipedia  \n",
       "532           리커창     11   wikitree  \n",
       "533           손병희     11  wikipedia  \n",
       "\n",
       "[534 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. 커스텀데이터셋 만들기\n",
    "\n",
    "- **MyDataset**이라는 커스텀 데이터셋을 만들어보고자 한다.\n",
    "- 구조를 최대한 추상화해보자면 `Iterable[Tuple[Iterable[int] , Iterable[int] , Iterable[int] , int]]`와 같은 구조를 보인다.\n",
    "- input_ids`(Iterable[int])`, token_type_ids`(Iteralbe[int])`, attention_mask`(Iterable[int])`, lable`(int)`을 튜플로 묶어 Iterable한 구조로 이은 것으로 이해하면 좋을 것 같다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MyDataset(torch.utils.data.Dataset ):\n",
    "    def __init__( self, df : pd.DataFrame,  train_mode :bool = False,) : # 학습용 셋인지 테스트용 셋인지를 판단하는 지표\n",
    "        assert type(df) == pd.core.frame.DataFrame\n",
    "\n",
    "        self.train_mode = train_mode\n",
    "        if train_mode :\n",
    "            self.class2idx = CLS2IDX\n",
    "            self.idx2class = IDX2CLS\n",
    "            # self.labels :Iterable[int] = df['label'].map(lambda x: self.class2idx[x]).values\n",
    "            self.labels = df['label']\n",
    "        sentence_list :List[str] = []\n",
    "\n",
    "        for idx in range(len(df)):\n",
    "            new_sentence :str = df.loc[idx,'subject_entity'] + \"[SEP]\"\n",
    "            new_sentence += df.loc[idx, 'object_entity'] + \"[SEP]\" \n",
    "            new_sentence += df.loc[idx, 'sentence' ]\n",
    "            sentence_list.append(new_sentence)\n",
    "        self.sentence_tensor_list :Dict[str , torch.Tensor] = tokenizer(\n",
    "            sentence_list, \n",
    "            padding=True, \n",
    "            # truncation=True, \n",
    "            return_tensors='pt',\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __getitem__(self, key :int): # -> Tuple[Any]:\n",
    "        segment_token_list = []\n",
    "        segment_token = 0\n",
    "        for token in self.sentence_tensor_list['input_ids'][key]:\n",
    "            segment_token_list.append(1 if segment_token==2 else 0)\n",
    "            if token.item() == 3 and segment_token <3 :\n",
    "                segment_token += 1\n",
    "            \n",
    "        segment_token_list = torch.LongTensor(segment_token_list)\n",
    "\n",
    "        return (\n",
    "            self.sentence_tensor_list['input_ids'][key],\n",
    "            segment_token_list, #self.sentence_tensor_list['token_type_ids'][key], \n",
    "            self.sentence_tensor_list['attention_mask'][key],\n",
    "            self.labels[key] if self.train_mode else None,\n",
    "        )\n",
    "    def __len__(self) :\n",
    "        return len(self.sentence_tensor_list['input_ids'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 스페셜 토큰들의 id들을 파악할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(tokenizer.all_special_ids, '\\n', tokenizer.all_special_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 3, 0, 2, 4] \n",
      " ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 간단한 예시를 통해 tokenizer와 MyDataset클래스가 잘 작동하는지 확인해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "exam_dataset = MyDataset(df=df, train_mode=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(exam_dataset[0][2][:].__len__())\n",
    "exam_dataset[10][3] # token_type_ids"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "229\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tokenizer.decode(exam_dataset[10][0]) #토큰화 -> 인코딩 -> 디코딩 의 절차를 걸친후의 결과"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] 하비에르 파스토레 [SEP] 아르헨티나 [SEP] 하비에르 파스토레는 아르헨티나 클럽 타예레스의 유소년팀에서 축구를 시작하였다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# exam_dataset.idx2class[exam_dataset[10][3]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#exam_dataset[10][0][:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. 데이터를 분할해주는 함수`(split_data)` 선언하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def split_data(i) :\n",
    "    assert type(i) == int\n",
    "    i = i % 10\n",
    "    assert 0<=i<=9 #i는 0~9사이의 숫자여야 한다.\n",
    "    train_data = pd.DataFrame()\n",
    "    valid_data = pd.DataFrame() \n",
    "    for label_idx in range(30):\n",
    "        batch_len = df_dict_len[label_idx]//10\n",
    "        train_data_temp = pd.concat([\n",
    "            df_dict[label_idx][ : i*batch_len ], \n",
    "            df_dict[label_idx][(i+1)*batch_len : ],\n",
    "        ]) # 어떤 클래스의 train 용 데이터\n",
    "        valid_data_temp = df_dict[label_idx][ i * batch_len : (i+1)*batch_len] # 어떤 클래스의 valid 용 데이터\n",
    "        train_data = pd.concat([train_data, train_data_temp]).reset_index(drop=True)\n",
    "        valid_data = pd.concat([valid_data, valid_data_temp]).reset_index(drop=True)\n",
    "    return MyDataset(train_data, train_mode=True) , MyDataset(valid_data, train_mode=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 학습\n",
    "## 3.1. auc를 계산해주는 함수`(cal_auc)` 선언하기\n",
    "- **odds** `(Iterable[Iterable[int]])` :\n",
    "    - (n, 30) 형태의 행렬이다. \n",
    "    - 각 행의 총합은 1이다. \n",
    "    - 열의 개수는 분류하고자 하는 클래스의 수를 의미한다.\n",
    "- **labels** `(Iterable[int])` : \n",
    "    - n차원 벡터이다.\n",
    "    - 각 entry는 정답 label을 의미한다.\n",
    "- **label_matrix** `(Iterable[Iterable[ 1|0 ]])` :\n",
    "    - labels를 원핫벡터로 표현한 행렬이다.\n",
    "    - (n, 30) 형태의 행렬이다.\n",
    "\n",
    "```python\n",
    "fpr, tpr, thresholds = roc_curve(\n",
    "    y_true= label_matrix[:, i] , \n",
    "    y_score= odds[:,i],\n",
    "    pos_label= 1,\n",
    ")\n",
    "auc(fpr, tpr)\n",
    "\n",
    "```\n",
    "- 위 코드는 label i를 positive class라고 가정했을 때,\n",
    "- 즉 i를 제외한 다른 class들은 negative class라고 가정했을 때,\n",
    "- False Positive Rate 대비 True Positive Rate들을 측정하고,\n",
    "- 이를 통해 label i를 positive라고 둘 때의 auc를 측정하는 코드이다.\n",
    "- `cal_auc`는 for문을 통해 30개의 label별로 auc를 계산해서 평균를 반환한다.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "def cal_auc(odds, labels):\n",
    "    odds = np.array(odds)\n",
    "    labels = np.array(labels)\n",
    "    #print(odds.shape)\n",
    "    #print(labels.shape)\n",
    "    label_matrix = np.zeros([ labels.__len__() , 30 ])\n",
    "    for row, column in enumerate(labels):\n",
    "        label_matrix[row, column] = 1\n",
    "    \n",
    "    # label_matrix[:,29] \n",
    "    auc_list = []\n",
    "    for i in range(30):\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "            y_true= label_matrix[:, i] , \n",
    "            y_score= odds[:,i],\n",
    "            pos_label= 1,\n",
    "        )\n",
    "        auc_list.append(auc(fpr, tpr))\n",
    "    auc_score =sum(auc_list)/len(auc_list)\n",
    "    return auc_score\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2. 학습 함수`(lets_train)` 선언하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def lets_train(\n",
    "    epoch, \n",
    "    BATCH_SIZE, \n",
    "    L_RATE,\n",
    "    LOGGING_POINT,\n",
    "):\n",
    "    print(\"배치사이즈 : \", BATCH_SIZE)\n",
    "    print(\"learning_rate : \", L_RATE)\n",
    "    print(\"사용된 모델 : \", MODEL_NAME)\n",
    "\n",
    "    train_dataset, valid_dataset = split_data(epoch)\n",
    "    \n",
    "    train_batch = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_batch = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    pred_list = []   #List[int]        n차원 벡터\n",
    "    label_list = []  #List[int]        n차원 벡터\n",
    "    odd_list = []    #List[List[int]]  (n,30) 행렬, \n",
    "                            # 1. 각 행의 합은 1, \n",
    "                            # 2.각 n행 m열은 \n",
    "                            # 3. n번째 데이터의 m번 클래스에 대해 부여한 확률\n",
    "    accumulattion_label_list = [] #List[int] label_list와 달리 마지막 이터레이션에서만 초기화됨.\n",
    "    for idx,( input_ids, token_type_ids, attention_mask, label ) in  enumerate( train_batch ) :\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(\n",
    "            input_ids=input_ids.to(device),\n",
    "            token_type_ids=token_type_ids.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "        ).logits\n",
    "\n",
    "\n",
    "        # data_batch 묶음에 대해  (batch_size를 n이라고 할 때)\n",
    "        # 모델이 추정한 label들의 리스트 // n차원 벡터\n",
    "        pred_list += list(torch.argmax(pred,dim=-1).cpu().numpy()) \n",
    "        \n",
    "        # 모델이 추정한 label별 확률 // (n,30) 행렬  \n",
    "        odd_list += list(softmax(pred).cpu().detach().numpy())\n",
    "        # 실제 데이터의 label 리스트 // n차원 벡터\n",
    "        label_list += list(label.numpy())\n",
    "\n",
    "        # 지금까지 모델이 추정해온 label들\n",
    "        accumulattion_label_list  += list(label.numpy())\n",
    "\n",
    "\n",
    "        loss = criterion(pred, label.to(device))\n",
    "        if idx%LOGGING_POINT == LOGGING_POINT-1 or idx ==  len(train_batch) - 1 :\n",
    "            print(\n",
    "                \"loss : \", loss.item(), \n",
    "                \"\\nf1score : {}\".format(f1_score(\n",
    "                    y_true=label_list, y_pred = pred_list, \n",
    "                    average ='micro', labels=range(1,30), \n",
    "                )),\n",
    "                \"\\nacc : {}\".format(accuracy_score(\n",
    "                    y_true=label_list, y_pred = pred_list\n",
    "                )),\n",
    "            )\n",
    "            pred_list =[]   #pred_list와 label_list는\n",
    "            label_list = [] #매 LOGGING_POINT마다 초기화\n",
    "        \n",
    "        # 아래는 auc를 구하는 코드 \n",
    "        if idx%LOGGING_POINT == LOGGING_POINT-1 or idx ==  len(train_batch) - 1 :\n",
    "            try : \n",
    "                print(f\"auc : {cal_auc(odds=odd_list, labels=accumulattion_label_list)}\")\n",
    "            except:\n",
    "                print(\n",
    "                    \"odd_list = \\n\", odd_list, \n",
    "                    \"accumulattion_label_list = \\n\", accumulattion_label_list \n",
    "                )\n",
    "\n",
    "        if idx ==  len(train_batch) - 1 :\n",
    "            odd_list = []                   # odd_list와 accumulation_label_list는\n",
    "            accumulattion_label_list = []   # 마지막 이터레이션에서 초기화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred_list = []\n",
    "        val_label_list = []\n",
    "        val_odd_list = []\n",
    "        val_accumulattion_label_list = []\n",
    "        for idx, (input_ids, token_type_ids, attention_mask, label) in  enumerate(valid_batch) :\n",
    "        \n",
    "            pred = model(\n",
    "                input_ids=input_ids.to(device),\n",
    "                token_type_ids=token_type_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "            ).logits\n",
    "\n",
    "            val_pred_list += list(torch.argmax(pred,dim=-1).cpu().numpy()) \n",
    "            val_label_list += list(label.numpy())\n",
    "            val_odd_list += list(softmax(pred).cpu().detach().numpy())\n",
    "            val_accumulattion_label_list += list(label.numpy())\n",
    "\n",
    "            if idx%LOGGING_POINT == LOGGING_POINT-1 or idx ==  len(valid_batch) - 1 :\n",
    "                print(\n",
    "                    f\"\\nval_f1score : {f1_score(y_true=val_label_list, y_pred = val_pred_list, average ='micro', labels=range(1,30))}\",\n",
    "                    f\"\\nval_acc : {accuracy_score(y_true=val_label_list, y_pred = val_pred_list)}\",\n",
    "                )\n",
    "                try:\n",
    "                    print(f\"auc : {cal_auc(odds=val_odd_list, labels=val_accumulattion_label_list)}\")\n",
    "                except:\n",
    "                    print(\n",
    "                        \"val_odd_list = \\n\", val_odd_list, \n",
    "                        \"val_accumulattion_label_list = \\n\", val_accumulattion_label_list ,\n",
    "                    )\n",
    "                val_pred_list =[]\n",
    "                val_label_list = []\n",
    "\n",
    "            if idx ==  len(valid_batch) - 1 :\n",
    "                try:\n",
    "                    print(f\"auc : {cal_auc(odds=val_odd_list, labels=val_accumulattion_label_list)}\")\n",
    "                except:\n",
    "                    print(\n",
    "                        \"val_odd_list = \\n\", val_odd_list, \n",
    "                        \"val_accumulattion_label_list = \\n\", val_accumulattion_label_list ,\n",
    "                    )\n",
    "                val_odd_list = []\n",
    "                val_accumulattion_label_list = []\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 50\n",
    "L_RATE = 0.000002\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=L_RATE )\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "model.to(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "if __name__ == \"__main__\":\n",
    "    print(f' 디바이스 : {device} \\n 모델 : {MODEL_NAME} {device} \\n 에포크 : {EPOCH} \\n 배치사이즈 : {BATCH_SIZE} \\n 러닝레이트 : {L_RATE}')\n",
    "    for ep in range(EPOCH):\n",
    "        print(ep+1, '번째 에폭')\n",
    "        lets_train(\n",
    "            epoch = ep, \n",
    "            BATCH_SIZE=BATCH_SIZE,\n",
    "            L_RATE=L_RATE,\n",
    "            LOGGING_POINT=100,\n",
    "        )\n",
    "        # if ep%5 == 4 : \n",
    "        #     torch.save(model, f'./cp/2nd_{ep}.pt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 디바이스 : cuda \n",
      " 모델 : kykim/bert-kor-base cuda \n",
      " 에포크 : 10 \n",
      " 배치사이즈 : 50 \n",
      " 러닝레이트 : 2e-06\n",
      "1 번째 에폭\n",
      "배치사이즈 :  50\n",
      "learning_rate :  2e-06\n",
      "사용된 모델 :  kykim/bert-kor-base\n",
      "loss :  2.7382442951202393 \n",
      "f1score : 0.01711326277865346 \n",
      "acc : 0.2546\n",
      "auc : 0.51283514850075\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12998/1167880725.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'번째 에폭'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         lets_train(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12998/1512631124.py\u001b[0m in \u001b[0;36mlets_train\u001b[0;34m(epoch, BATCH_SIZE, L_RATE, LOGGING_POINT)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0modd_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# odd_list와 accumulation_label_list는\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0maccumulattion_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# 마지막 이터레이션에서 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}