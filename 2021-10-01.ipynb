{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\r\n",
    "import torch\r\n",
    "from typing import Dict, Iterable, List, Any, Tuple\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "from random import randint\r\n",
    "from sklearn.metrics import f1_score, accuracy_score\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "MODEL_NAME = 'kykim/bert-kor-base'\r\n",
    "L_RATE = 0.001\r\n",
    "BATCH_SIZE = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\n",
    "model :torch.nn.Module= AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
    "model.bert.embeddings.position_embeddings = torch.nn.Embedding(128, 768)\r\n",
    "model.bert.embeddings.token_type_embeddings = torch.nn.Embedding(3, 768)\r\n",
    "model.classifier = torch.nn.Linear(768, 30, bias=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "df = pd.read_csv('../dataset/train/train.csv')\r\n",
    "df['subject_entity'] = df['subject_entity'].map(lambda x: eval(x)['word'])\r\n",
    "df['object_entity'] = df['object_entity'].map(lambda x: eval(x)['word'])\r\n",
    "df.sample(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30689</th>\n",
       "      <td>30689</td>\n",
       "      <td>소크라치스 브라질레이루 삼파이우 지 소자 비에이라 지 올리베이라(1954년 2월 1...</td>\n",
       "      <td>소크라치스</td>\n",
       "      <td>2011년 12월 4일</td>\n",
       "      <td>per:date_of_death</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19189</th>\n",
       "      <td>19189</td>\n",
       "      <td>해당 웹툰을 접한 커뮤니티 이용자들은 \"웹툰 그리라고 했더니 혼자 '스튜디오 지브리...</td>\n",
       "      <td>미야자키 하야오</td>\n",
       "      <td>모노노케 히메</td>\n",
       "      <td>per:product</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>1478</td>\n",
       "      <td>한편 대한민국 정부는 문화체육관광부 와 한국관광공사 가 대한민국 내 관광산업의 지원...</td>\n",
       "      <td>한국관광공사</td>\n",
       "      <td>문화체육관광부</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14880</th>\n",
       "      <td>14880</td>\n",
       "      <td>둘 다 프랑스 자동차경주 연맹의 지원을 받았고 세계 랠리 선수권대회의 1600cc카...</td>\n",
       "      <td>시트로앵</td>\n",
       "      <td>자동차</td>\n",
       "      <td>org:product</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>29563</td>\n",
       "      <td>그녀의 대모(代母)는 당시 스페인 왕비 바텐베르크의 빅토리아 에우헤니아였다.</td>\n",
       "      <td>바텐베르크의 빅토리아 에우헤니아</td>\n",
       "      <td>스페인 왕비</td>\n",
       "      <td>per:title</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "30689  30689  소크라치스 브라질레이루 삼파이우 지 소자 비에이라 지 올리베이라(1954년 2월 1...   \n",
       "19189  19189  해당 웹툰을 접한 커뮤니티 이용자들은 \"웹툰 그리라고 했더니 혼자 '스튜디오 지브리...   \n",
       "1478    1478  한편 대한민국 정부는 문화체육관광부 와 한국관광공사 가 대한민국 내 관광산업의 지원...   \n",
       "14880  14880  둘 다 프랑스 자동차경주 연맹의 지원을 받았고 세계 랠리 선수권대회의 1600cc카...   \n",
       "29563  29563         그녀의 대모(代母)는 당시 스페인 왕비 바텐베르크의 빅토리아 에우헤니아였다.   \n",
       "\n",
       "          subject_entity object_entity              label     source  \n",
       "30689              소크라치스  2011년 12월 4일  per:date_of_death  wikipedia  \n",
       "19189           미야자키 하야오       모노노케 히메        per:product   wikitree  \n",
       "1478              한국관광공사       문화체육관광부        no_relation  wikipedia  \n",
       "14880               시트로앵           자동차        org:product  wikipedia  \n",
       "29563  바텐베르크의 빅토리아 에우헤니아        스페인 왕비          per:title  wikipedia  "
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "type(df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "class MyDataset(torch.utils.data.Dataset ):\r\n",
    "    def __init__(\r\n",
    "        self, \r\n",
    "        df : pd.DataFrame,\r\n",
    "        train_mode :bool = False,  # 학습용 셋인지 테스트용 셋인지를 판단하는 지표\r\n",
    "    )->None :\r\n",
    "        assert type(df) == pd.core.frame.DataFrame\r\n",
    "\r\n",
    "        self.train_mode = train_mode\r\n",
    "        if train_mode :\r\n",
    "            self.idx2class :Dict[int, str]= { \r\n",
    "                idx : classss for idx, classss in enumerate(\r\n",
    "                    df['label'].unique()\r\n",
    "                )\r\n",
    "            }\r\n",
    "            self.class2idx = {\r\n",
    "                classss : idx for idx, classss in self.idx2class.items()\r\n",
    "            }    \r\n",
    "            self.labels :Iterable[int] = df['label'].map(lambda x: self.class2idx[x]).values\r\n",
    "    \r\n",
    "\r\n",
    "        sentence_list :List[str] = []\r\n",
    "\r\n",
    "        for idx in range(len(df)):\r\n",
    "            new_sentence :str = df.loc[idx,'subject_entity'] + \"[SEP]\"\r\n",
    "            new_sentence += df.loc[idx, 'object_entity'] + \"[SEP]\" \r\n",
    "            new_sentence += df.loc[idx, 'sentence' ]\r\n",
    "            sentence_list.append(new_sentence)\r\n",
    "        self.sentence_tensor_list :Dict[str , torch.Tensor] = tokenizer(\r\n",
    "            sentence_list, \r\n",
    "            padding=True, \r\n",
    "            truncation=True, \r\n",
    "            return_tensors='pt',\r\n",
    "            max_length=128\r\n",
    "        )\r\n",
    "\r\n",
    "        \r\n",
    "    def __getitem__(self, key :int) -> Tuple[Any]:\r\n",
    "        segment_token_list = []\r\n",
    "        segment_token = 0\r\n",
    "        for token in self.sentence_tensor_list['input_ids'][key]:\r\n",
    "            segment_token_list.append(segment_token)\r\n",
    "            if token.item() == 3 and segment_token <2 :\r\n",
    "                segment_token += 1\r\n",
    "            \r\n",
    "        segment_token_list = torch.LongTensor(segment_token_list)\r\n",
    "\r\n",
    "        return (\r\n",
    "            self.sentence_tensor_list['input_ids'][key],\r\n",
    "            segment_token_list, #self.sentence_tensor_list['token_type_ids'][key], \r\n",
    "            self.sentence_tensor_list['attention_mask'][key],\r\n",
    "            self.labels[key] if self.train_mode else None,\r\n",
    "        )\r\n",
    "    def __len__(self) :\r\n",
    "        return len(self.sentence_tensor_list['input_ids'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# exam_dataset = MyDataset(df=df, train_mode=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# exam_dataset[10][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# exam_dataset[10][1]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "\r\n",
    "def split_data():\r\n",
    "    TOTAL_DATA_NUM :int = df.__len__()\r\n",
    "\r\n",
    "    oob = list(range(TOTAL_DATA_NUM))\r\n",
    "    train_idx = []\r\n",
    "    for _ in range(TOTAL_DATA_NUM): # 32470\r\n",
    "        extracted = randint(0, TOTAL_DATA_NUM)\r\n",
    "        if extracted in oob :\r\n",
    "            oob[extracted] = None\r\n",
    "            train_idx.append(extracted)\r\n",
    "        \r\n",
    "    new_oob = []\r\n",
    "    for x in oob :\r\n",
    "        if x :\r\n",
    "            new_oob.append(x)\r\n",
    "    oob = new_oob\r\n",
    "\r\n",
    "    valid_dataset = MyDataset(\r\n",
    "        df=df.loc[ oob , : ].reset_index( drop=True),\r\n",
    "        train_mode=True\r\n",
    "    )\r\n",
    "    train_dataset = MyDataset(\r\n",
    "        df=df.loc[ train_idx , : ].reset_index( drop=True),\r\n",
    "        train_mode=True\r\n",
    "    )\r\n",
    "\r\n",
    "    train_batch = torch.utils.data.DataLoader(\r\n",
    "        train_dataset,\r\n",
    "        batch_size = BATCH_SIZE,\r\n",
    "        shuffle=True,\r\n",
    "        drop_last=True,\r\n",
    "        num_workers=4\r\n",
    "    )\r\n",
    "\r\n",
    "    valid_batch = torch.utils.data.DataLoader(\r\n",
    "        valid_dataset,\r\n",
    "        batch_size = BATCH_SIZE,\r\n",
    "        shuffle=True,\r\n",
    "        drop_last=True,\r\n",
    "        num_workers=4\r\n",
    "    )\r\n",
    "\r\n",
    "    return train_batch, valid_batch\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=L_RATE )\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "model.to(device)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 768)\n",
       "      (token_type_embeddings): Embedding(3, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# train, valid = split_data()\n",
    "# for x in train:\n",
    "#     print(x[1][0:2].dtype)\n",
    "#     print(x[0][0:2].dtype)\n",
    "\n",
    "#     break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "def lets_train():\n",
    "    train_batch, valid_batch = split_data()\n",
    "\n",
    "    model.train()\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    for idx,( input_ids, token_type_ids, attention_mask, label ) in tqdm( enumerate( train_batch ) ):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(\n",
    "            input_ids=input_ids.to(device),\n",
    "            token_type_ids=token_type_ids.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "        ).logits\n",
    "        # print(pred, label)\n",
    "        \n",
    "        pred_list += list(torch.argmax(pred,dim=-1).cpu().numpy()) \n",
    "        label_list += list(label.numpy())\n",
    "        loss = criterion(pred, label.to(device))\n",
    "        if idx % 100 ==0 :\n",
    "            print(\n",
    "                \"loss : \", \n",
    "                loss.item(), \n",
    "                f\"\\nf1score : {f1_score(y_true=label_list, y_pred = pred_list, average ='macro')}\",\n",
    "                f\"\\nacc : {accuracy_score(y_true=label_list, y_pred = pred_list)}\"\n",
    "            )\n",
    "        # break    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        for idx, (input_ids, token_type_ids, attention_mask, label) in tqdm( enumerate(valid_batch) ):\n",
    "        \n",
    "            pred = model(\n",
    "                input_ids=input_ids.to(device),\n",
    "                token_type_ids=token_type_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "            ).logits\n",
    "\n",
    "            pred_list += list(torch.argmax(pred,dim=-1).cpu().numpy()) \n",
    "            label_list += list(label.numpy())\n",
    "            if idx % 100 ==0 :\n",
    "                print(\n",
    "                # \"loss : \", \n",
    "                # loss.item(), \n",
    "                f\"\\nf1score : {f1_score(y_true=label_list, y_pred = pred_list, average ='macro')}\",\n",
    "                f\"\\nacc : {accuracy_score(y_true=label_list, y_pred = pred_list)}\"\n",
    "\n",
    "                )\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 100\n",
    "if __name__ == \"__main__\":\n",
    "    for ep in range(EPOCH):\n",
    "        print(ep+1, '번째 에폭')\n",
    "        lets_train()\n",
    "        torch.save(model, f'./2021-10-02_BERT_Test({ep+1}_{EPOCH}).pt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 번째 에폭\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss :  3.4718167781829834 \n",
      "f1score : 0.006238185255198488 \n",
      "acc : 0.03\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "64it [00:44,  1.43it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14432/4090097017.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'번째 에폭'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlets_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'./2021-10-02_2번째({ep+1}_{EPOCH}).pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14432/1281518222.py\u001b[0m in \u001b[0;36mlets_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# print(pred, label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpred_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlabel_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.all_special_tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "metadata": {},
     "execution_count": 266
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.all_special_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 3, 0, 2, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save_pretrained(save_directory='./')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}