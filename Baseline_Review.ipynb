{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle as pickle\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "#from load_data import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Load"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train = pd.read_csv(\"../dataset/train/train.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "MODEL_NAME = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    subject_entity = []\n",
    "    object_entity = []\n",
    "    for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "        i = i[1:-1].split(',')[0].split(':')[1]\n",
    "        j = j[1:-1].split(',')[0].split(':')[1]\n",
    "\n",
    "        subject_entity.append(i)\n",
    "        object_entity.append(j)\n",
    "    out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})\n",
    "    return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    dataset = preprocessing_dataset(pd_dataset)\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "train_dataset = load_data(\"../dataset/train/train.csv\")\n",
    "train_dataset[:3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ã€ˆSomethingã€‰ëŠ” ì¡°ì§€ í•´ë¦¬ìŠ¨ì´ ì“°ê³  ë¹„í‹€ì¦ˆê°€ 1969ë…„ ì•¨ë²” ã€ŠAbbey R...</td>\n",
       "      <td>'ë¹„í‹€ì¦ˆ'</td>\n",
       "      <td>'ì¡°ì§€ í•´ë¦¬ìŠ¨'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>í˜¸ë‚¨ì´ ê¸°ë°˜ì¸ ë°”ë¥¸ë¯¸ë˜ë‹¹Â·ëŒ€ì•ˆì‹ ë‹¹Â·ë¯¼ì£¼í‰í™”ë‹¹ì´ ìš°ì—¬ê³¡ì ˆ ëì— í•©ë‹¹í•´ ë¯¼ìƒë‹¹(ê°€ì¹­)ìœ¼...</td>\n",
       "      <td>'ë¯¼ì£¼í‰í™”ë‹¹'</td>\n",
       "      <td>'ëŒ€ì•ˆì‹ ë‹¹'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kë¦¬ê·¸2ì—ì„œ ì„±ì  1ìœ„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” ê´‘ì£¼FCëŠ” ì§€ë‚œ 26ì¼ í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹ìœ¼ë¡œë¶€í„°...</td>\n",
       "      <td>'ê´‘ì£¼FC'</td>\n",
       "      <td>'í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹'</td>\n",
       "      <td>org:member_of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence subject_entity  \\\n",
       "0   0  ã€ˆSomethingã€‰ëŠ” ì¡°ì§€ í•´ë¦¬ìŠ¨ì´ ì“°ê³  ë¹„í‹€ì¦ˆê°€ 1969ë…„ ì•¨ë²” ã€ŠAbbey R...          'ë¹„í‹€ì¦ˆ'   \n",
       "1   1  í˜¸ë‚¨ì´ ê¸°ë°˜ì¸ ë°”ë¥¸ë¯¸ë˜ë‹¹Â·ëŒ€ì•ˆì‹ ë‹¹Â·ë¯¼ì£¼í‰í™”ë‹¹ì´ ìš°ì—¬ê³¡ì ˆ ëì— í•©ë‹¹í•´ ë¯¼ìƒë‹¹(ê°€ì¹­)ìœ¼...        'ë¯¼ì£¼í‰í™”ë‹¹'   \n",
       "2   2  Kë¦¬ê·¸2ì—ì„œ ì„±ì  1ìœ„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” ê´‘ì£¼FCëŠ” ì§€ë‚œ 26ì¼ í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹ìœ¼ë¡œë¶€í„°...         'ê´‘ì£¼FC'   \n",
       "\n",
       "  object_entity          label  \n",
       "0      'ì¡°ì§€ í•´ë¦¬ìŠ¨'    no_relation  \n",
       "1        'ëŒ€ì•ˆì‹ ë‹¹'    no_relation  \n",
       "2    'í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹'  org:member_of  "
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('../code/dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "    return num_label\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "train_label[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 0, 20, 1, 0, 5, 0, 25, 7, 6]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- no_relation -> 0 ... label ì„ ìˆ«ìë¡œ ë§¤ì¹­í•´ì¤Œ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data tokenized"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      )\n",
    "  return tokenized_sentences\n",
    "\n",
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print(tokenized_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[    2,    11, 29830,  ...,     0,     0,     0],\n",
      "        [    2,    11,  3772,  ...,     0,     0,     0],\n",
      "        [    2,    11,  4104,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,    11, 18272,  ...,     0,     0,     0],\n",
      "        [    2,    11, 15710,  ...,     0,     0,     0],\n",
      "        [    2,    11, 15437,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Dataset êµ¬ì„±ì„ ìœ„í•œ class\n",
    "# make dataset for pytorch.\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, labels):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 30\n",
    "model_config"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForPretraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\",\n",
       "    \"15\": \"LABEL_15\",\n",
       "    \"16\": \"LABEL_16\",\n",
       "    \"17\": \"LABEL_17\",\n",
       "    \"18\": \"LABEL_18\",\n",
       "    \"19\": \"LABEL_19\",\n",
       "    \"20\": \"LABEL_20\",\n",
       "    \"21\": \"LABEL_21\",\n",
       "    \"22\": \"LABEL_22\",\n",
       "    \"23\": \"LABEL_23\",\n",
       "    \"24\": \"LABEL_24\",\n",
       "    \"25\": \"LABEL_25\",\n",
       "    \"26\": \"LABEL_26\",\n",
       "    \"27\": \"LABEL_27\",\n",
       "    \"28\": \"LABEL_28\",\n",
       "    \"29\": \"LABEL_29\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_15\": 15,\n",
       "    \"LABEL_16\": 16,\n",
       "    \"LABEL_17\": 17,\n",
       "    \"LABEL_18\": 18,\n",
       "    \"LABEL_19\": 19,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_20\": 20,\n",
       "    \"LABEL_21\": 21,\n",
       "    \"LABEL_22\": 22,\n",
       "    \"LABEL_23\": 23,\n",
       "    \"LABEL_24\": 24,\n",
       "    \"LABEL_25\": 25,\n",
       "    \"LABEL_26\": 26,\n",
       "    \"LABEL_27\": 27,\n",
       "    \"LABEL_28\": 28,\n",
       "    \"LABEL_29\": 29,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "model =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "model.parameters\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    save_total_limit=5,              # number of total save model.\n",
    "    save_steps= 500,                 # model saving step.\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    learning_rate= 5e-6, #5e-5,               # learning_rate\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps= 500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,              # log saving step.\n",
    "    evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                                # `no`: No evaluation during training.\n",
    "                                # `steps`: Evaluate every `eval_steps`.\n",
    "                                # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps = 500,            # evaluation step.\n",
    "    load_best_model_at_end = True \n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def klue_re_micro_f1(preds, labels):\n",
    "  label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "      'org:product', 'per:title', 'org:alternate_names',\n",
    "      'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "      'org:number_of_employees/members', 'per:children',\n",
    "      'per:place_of_residence', 'per:alternate_names',\n",
    "      'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "      'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "      'org:member_of', 'per:parents', 'org:dissolved',\n",
    "      'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "      'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "      'per:religion']\n",
    "  no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "  label_indices = list(range(len(label_list)))\n",
    "  label_indices.remove(no_relation_label_idx)\n",
    "  return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "  labels = np.eye(30)[labels]\n",
    "\n",
    "  score = np.zeros((30,))\n",
    "  for c in range(30):\n",
    "    targets_c = labels.take([c], axis=1).ravel()\n",
    "    preds_c = probs.take([c], axis=1).ravel()\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "    score[c] = sklearn.metrics.auc(recall, precision)\n",
    "  return np.average(score) * 100.0\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = klue_re_micro_f1(preds, labels)\n",
    "  auprc = klue_re_auprc(probs, labels)\n",
    "  acc = accuracy_score(labels, preds) # ë¦¬ë”ë³´ë“œ í‰ê°€ì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "  return {\n",
    "      'micro f1 score': f1,\n",
    "      'auprc' : auprc,\n",
    "      'accuracy': acc,\n",
    "  }\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "  args=training_args,                  # training arguments, defined above\n",
    "  train_dataset=RE_train_dataset,         # training dataset\n",
    "  eval_dataset=RE_train_dataset,             # evaluation dataset\n",
    "  compute_metrics=compute_metrics         # define metrics function\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train code\n",
    "# trainer.train()\n",
    "# model.save_pretrained('./best_model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## inference.py"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  \n",
    "  # model dir\n",
    "  parser.add_argument('--model_dir', type=str, default=\"./best_model\")\n",
    "  args = parser.parse_args()\n",
    "  print(args)\n",
    "  main(args)\n",
    "```\n",
    "\n",
    "- parser ì§„í–‰ í›„ main ë¬¸ ì‹¤í–‰"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# parser.add_argument('--model_dir', type=str, default=\"./best_model\") \n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model_dir', type=str, default=\"./best_model\") \n",
    "\n",
    "# python inference.py --model_dir=./results/checkpoint-500\n",
    "# ëª¨ë¸ ê²½ë¡œ ì„¤ì •í•˜ì—¬ model ì„ íƒ\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--model_dir'], dest='model_dir', nargs=None, const=None, default='./best_model', type=<class 'str'>, choices=None, help=None, metavar=None)"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "!python ../code/inference.py --model_dir=./results/checkpoint-40500"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Namespace(model_dir='./results/checkpoint-40500')\n",
      "404 Client Error: Not Found for url: https://huggingface.co/results/checkpoint-40500/resolve/main/config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 524, in get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\", line 1404, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\", line 1575, in get_from_cache\n",
      "    r.raise_for_status()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/requests/models.py\", line 941, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/results/checkpoint-40500/resolve/main/config.json\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"../code/inference.py\", line 101, in <module>\n",
      "    main(args)\n",
      "  File \"../code/inference.py\", line 73, in main\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(args.model_dir)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 382, in from_pretrained\n",
      "    config, kwargs = AutoConfig.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 515, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 548, in get_config_dict\n",
      "    raise EnvironmentError(msg)\n",
      "OSError: Can't load config for './results/checkpoint-40500'. Make sure that:\n",
      "\n",
      "- './results/checkpoint-40500' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or './results/checkpoint-40500' is the correct path to a directory containing a config.json file\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ì…ë ¥ë°›ì€ ì¸ìê°’ ì €ì¥\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "train_dataset[439:442]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>439</td>\n",
       "      <td>ë®¤ì§€ì»¬ ë°°ìš° í•¨ì—°ì§€ ì”¨ê°€ 'ë™ìƒì´ëª½'ì— ì¶œì—°í–ˆë‹¤.</td>\n",
       "      <td>'í•¨ì—°ì§€'</td>\n",
       "      <td>'ë®¤ì§€ì»¬ ë°°ìš°'</td>\n",
       "      <td>per:title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>440</td>\n",
       "      <td>21ì„¸ì˜ ì•™ê°± ê³µì‘ ë£¨ì´ê°€ ì§€íœ˜í•˜ëŠ” í”„ë‘ìŠ¤êµ°ì€ ì¬ë¹ ë¥´ê²Œ ìŠ¤í˜ì¸ êµ°ì˜ ì›€ì§ì„ì— ë°˜ì‘í•˜...</td>\n",
       "      <td>'ìŠ¤í˜ì¸'</td>\n",
       "      <td>'6</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>441</td>\n",
       "      <td>1984ë…„ 5ì›” ë¹Œë°”ì˜¤ì™€ì˜ ì½”íŒŒ ë¸ ë ˆì´(ìŠ¤í˜ì¸ êµ­ì™•ì»µ) ê²°ìŠ¹ì—ì„œ ë§ˆë¼ë„ë‚˜ëŠ” ì§‘ë‹¨ ...</td>\n",
       "      <td>'ì½”íŒŒ ë¸ ë ˆì´'</td>\n",
       "      <td>'ìŠ¤í˜ì¸'</td>\n",
       "      <td>org:place_of_headquarters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           sentence subject_entity  \\\n",
       "439  439                        ë®¤ì§€ì»¬ ë°°ìš° í•¨ì—°ì§€ ì”¨ê°€ 'ë™ìƒì´ëª½'ì— ì¶œì—°í–ˆë‹¤.          'í•¨ì—°ì§€'   \n",
       "440  440  21ì„¸ì˜ ì•™ê°± ê³µì‘ ë£¨ì´ê°€ ì§€íœ˜í•˜ëŠ” í”„ë‘ìŠ¤êµ°ì€ ì¬ë¹ ë¥´ê²Œ ìŠ¤í˜ì¸ êµ°ì˜ ì›€ì§ì„ì— ë°˜ì‘í•˜...          'ìŠ¤í˜ì¸'   \n",
       "441  441  1984ë…„ 5ì›” ë¹Œë°”ì˜¤ì™€ì˜ ì½”íŒŒ ë¸ ë ˆì´(ìŠ¤í˜ì¸ êµ­ì™•ì»µ) ê²°ìŠ¹ì—ì„œ ë§ˆë¼ë„ë‚˜ëŠ” ì§‘ë‹¨ ...      'ì½”íŒŒ ë¸ ë ˆì´'   \n",
       "\n",
       "    object_entity                      label  \n",
       "439      'ë®¤ì§€ì»¬ ë°°ìš°'                  per:title  \n",
       "440            '6                no_relation  \n",
       "441         'ìŠ¤í˜ì¸'  org:place_of_headquarters  "
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "train_dataset[13312:13314]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>13312</td>\n",
       "      <td>ê¹€ì˜ì‚¼ ì˜ì› ì œëª… íŒŒë™ì€ 1979ë…„ 9ì›” 29ì¼ ë¯¼ì£¼ê³µí™”ë‹¹ê³¼ ìœ ì‹ ì •ìš°íšŒì—ì„œ ì‹ ë¯¼ë‹¹ ...</td>\n",
       "      <td>'ë°•ì •í¬'</td>\n",
       "      <td>'ë¯¼ì£¼ê³µí™”ë‹¹'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13313</th>\n",
       "      <td>13313</td>\n",
       "      <td>ì–¼ë§ˆ ë’¤ ì•„ë¸Œë“œ ì—˜ í¬ë¦¼ì€ ì•„ëˆ„ì•Œ(Annual)ì˜ ìŠ¤í˜ì¸ ë³´ë³‘ëŒ€ë¥¼ ê¸‰ìŠµí•˜ì—¬ ë‹¨ 3,...</td>\n",
       "      <td>'ìŠ¤í˜ì¸'</td>\n",
       "      <td>'8</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "13312  13312  ê¹€ì˜ì‚¼ ì˜ì› ì œëª… íŒŒë™ì€ 1979ë…„ 9ì›” 29ì¼ ë¯¼ì£¼ê³µí™”ë‹¹ê³¼ ìœ ì‹ ì •ìš°íšŒì—ì„œ ì‹ ë¯¼ë‹¹ ...   \n",
       "13313  13313  ì–¼ë§ˆ ë’¤ ì•„ë¸Œë“œ ì—˜ í¬ë¦¼ì€ ì•„ëˆ„ì•Œ(Annual)ì˜ ìŠ¤í˜ì¸ ë³´ë³‘ëŒ€ë¥¼ ê¸‰ìŠµí•˜ì—¬ ë‹¨ 3,...   \n",
       "\n",
       "      subject_entity object_entity        label  \n",
       "13312          'ë°•ì •í¬'       'ë¯¼ì£¼ê³µí™”ë‹¹'  no_relation  \n",
       "13313          'ìŠ¤í˜ì¸'            '8  no_relation  "
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "train[13313:13314]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13313</th>\n",
       "      <td>13313</td>\n",
       "      <td>ì–¼ë§ˆ ë’¤ ì•„ë¸Œë“œ ì—˜ í¬ë¦¼ì€ ì•„ëˆ„ì•Œ(Annual)ì˜ ìŠ¤í˜ì¸ ë³´ë³‘ëŒ€ë¥¼ ê¸‰ìŠµí•˜ì—¬ ë‹¨ 3,...</td>\n",
       "      <td>{'word': 'ìŠ¤í˜ì¸', 'start_idx': 73, 'end_idx': 75...</td>\n",
       "      <td>{'word': '8,000ëª…', 'start_idx': 65, 'end_idx':...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "13313  13313  ì–¼ë§ˆ ë’¤ ì•„ë¸Œë“œ ì—˜ í¬ë¦¼ì€ ì•„ëˆ„ì•Œ(Annual)ì˜ ìŠ¤í˜ì¸ ë³´ë³‘ëŒ€ë¥¼ ê¸‰ìŠµí•˜ì—¬ ë‹¨ 3,...   \n",
       "\n",
       "                                          subject_entity  \\\n",
       "13313  {'word': 'ìŠ¤í˜ì¸', 'start_idx': 73, 'end_idx': 75...   \n",
       "\n",
       "                                           object_entity        label  \\\n",
       "13313  {'word': '8,000ëª…', 'start_idx': 65, 'end_idx':...  no_relation   \n",
       "\n",
       "          source  \n",
       "13313  wikipedia  "
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "!pip install datasets"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270 kB 14.0 MB/s \n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123 kB 90.8 MB/s \n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243 kB 70.2 MB/s \n",
      "\u001b[?25hCollecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76 kB 9.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.1.5)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86 kB 11.3 MB/s \n",
      "\u001b[?25hCollecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-5.0.0-cp38-cp38-manylinux2014_x86_64.whl (23.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.6 MB 65.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.19.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.24.0)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.7.4.post0-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5 MB 67.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.0.17)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128 kB 91.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2020.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159 kB 85.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (3.7.4.3)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 324 kB 74.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: multidict, yarl, async-timeout, aiohttp, fsspec, xxhash, tqdm, dill, pyarrow, multiprocess, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.51.0\n",
      "    Uninstalling tqdm-4.51.0:\n",
      "      Successfully uninstalled tqdm-4.51.0\n",
      "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 dill-0.3.4 fsspec-2021.9.0 multidict-5.1.0 multiprocess-0.70.12.2 pyarrow-5.0.0 tqdm-4.62.3 xxhash-2.0.2 yarl-1.6.3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from datasets import load_dataset\n",
    "klue_re = load_dataset(\"load_klue_re.py\")\n",
    "klue_re"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /opt/ml/klue-level2-nlp-18/load_klue_re.py",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-180ac3e2f7cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mklue_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_klue_re.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mklue_re\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, script_version, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \"\"\"\n\u001b[1;32m    903\u001b[0m     \u001b[0;31m# Download/copy dataset processing script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m     module_path, hash, base_path = prepare_module(\n\u001b[0m\u001b[1;32m    905\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mscript_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscript_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mprepare_module\u001b[0;34m(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, return_associated_base_path, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find a {resource_type} script at {relative_to_absolute_path(path)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /opt/ml/klue-level2-nlp-18/load_klue_re.py"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}