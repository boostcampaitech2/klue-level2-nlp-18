def train() : 

    def model_init():  ###train 함수 안에 넣어주어여 합니다
        return model

    trainer = Trainer(    
        model_init = model_init,       #기존에 modeld을 model_init으로 바꾸어야합니다  # the instantiated 🤗 Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=RE_train_dataset,         # training dataset
        eval_dataset=RE_train_dataset,             # evaluation dataset
        compute_metrics=compute_metrics  # define metrics function
            
)
    ###Optuna 환경설정 : 파라미터 값의 범위를 지정할 수 있습니다
    def optuna_hp_space(trial):
        return {
        "random_state": trial.suggest_float('random_state', 18 , 42),
        "save_steps" : 500,          # model saving step.
        "num_train_epochs" : trial.suggest_int('num_train_epochs', 3, 6),         # total number of training epochs
        "learning_rate" : trial.suggest_float('learning_rate', 5e-5, 5e-4),               # learning_rate
        "per_device_train_batch_size" : trial.suggest_int('train_batch_size', 16, 64),  # batch size per device during training
        "per_device_eval_batch_size" : trial.suggest_int('dev_batch_size', 16, 64) ,   # batch size for evaluation
        "warmup_steps" : trial.suggest_int('warmup_steps', 100, 1000),                # number of warmup steps for learning rate scheduler
        "weight_decay" : trial.suggest_int('weight_decay', 0.005, 0.05),               # strength of weight decay
            # directory for storing logs
        # log saving step.
        
        }
    ##Hugging face에서는 hyperparameter_search 모듈을 통해서 이를 지원해줍니다. n_trial은 전체 epoch를 돌린
    # 탐색을 통해 최적값을 찾아내면 이를 가지고 다시 한번 학습합니다.
    trainer.hyperparameter_search(
        direction="maximize", # NOTE: or direction="minimize"
        hp_space=optuna_hp_space, # NOTE: if you wanna use optuna, change it to optuna_hp_space
        n_trials = 2,
        #backend="ray", # NOTE: if you wanna use optuna, remove this argument
    )

    trianer.train()


